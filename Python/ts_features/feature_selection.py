import pandas as pdimport numpy as npfrom scipy.optimize import minimizefrom scipy.stats import kstestfrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAimport osimport matplotlib.pyplot as pltfrom adjustText import adjust_textfrom sklearn.feature_selection import f_regression, f_classifimport pyomo.opt as pyooimport pyomo.environ as pyoimport shutilfrom statsmodels.stats.outliers_influence import variance_inflation_factorfrom statsmodels.tools.tools import add_constantimport warnings'''Feature selection methods including VIF, correlation, Kolmogorv-Smirnov, PCA, and Information Theoretic MRMR'''def multicorr_coefs(X):    with warnings.catch_warnings():        warnings.filterwarnings("ignore", message="divide by zero encountered in double_scalars")        if len(X.columns) > 1:            vifs = pd.Series([variance_inflation_factor(add_constant(X).values, i)                               for i in range(1, len(X.columns)+1)],                              index=X.columns)            multicorr_r = np.sqrt(1 - (vifs ** -1))        else:            multicorr_r = pd.Series([0], index=X.columns)    return multicorr_r.sort_values(ascending=False)def single_multicorr_coef(X, i):    vif = variance_inflation_factor(add_constant(X).values, i+1)    multicorr_r = np.sqrt(1 - (vif ** -1))    return multicorr_r# Iteratively (greedy) removes features with max multicorrelation # until all multicorrelations < max_corr# TODO: rewrite as global optimization to find the largest subset of featuresdef select_non_multicollinear(X, max_corr=0.8):    X_sub = X.copy()    corr_coefs = multicorr_coefs(X_sub)    while len(corr_coefs) > 1 and corr_coefs.iloc[0] > max_corr:        X_sub = X_sub.drop(columns=[corr_coefs.index[0]])        corr_coefs = multicorr_coefs(X_sub)    features_to_keep = X_sub.columns    features_to_drop = X.columns[~X.columns.isin(X_sub.columns)]    print(f'Select Non-Multicollinear: Keeping {len(features_to_keep)} / {len(X.columns)} features with multiple correlation <= {round(corr_coefs.iloc[0], 2)}')    return features_to_keep, features_to_drop# Selects the largest subset of features # such that all selected features have correlation <= max_corrdef select_non_collinear(X, max_corr=0.8, verbose=False):    abs_corr_mat = X.corr().abs().copy()    np.fill_diagonal(abs_corr_mat.values, 0)    model = pyo.ConcreteModel()    model.F = pyo.Set(initialize=X.columns.values)    model.x = pyo.Var(model.F, within=pyo.Binary)    model.v = pyo.Param(model.F, model.F, initialize={x : y for x, y in abs_corr_mat.stack().items()})        def len_feature_subset(model):        return sum(model.x[i] for i in model.F)    model.obj = pyo.Objective(rule=len_feature_subset, sense=pyo.maximize)        def multicollinearity_rule(model, i, j):        value = model.v[i,j] * (model.x[i] + model.x[j] - 1)        return pyo.inequality(-1, value, max_corr)    model.multicollinearity_limit = pyo.Constraint(model.F, model.F, rule=multicollinearity_rule)    solverpath_exe = shutil.which('glpsol')    res = pyoo.SolverFactory('glpk', executable=solverpath_exe).solve(model)        keep_feature = pd.Series(model.x.extract_values()).astype(bool)    features_to_keep, features_to_drop = X.columns[keep_feature], X.columns[~keep_feature]    if not (res.solver.status == pyoo.SolverStatus.ok) or not (res.solver.termination_condition == pyoo.TerminationCondition.optimal):        res.write()    res_max_corr = abs_corr_mat.loc[features_to_keep, features_to_keep].max().max()    if res_max_corr > max_corr:        raise RuntimeError('The solution set max multicollinearity is greater than the input max multicollinearity')    print(f'Select Non-Collinear: Keeping {len(features_to_keep)} / {len(X.columns)} features with correlation <= {round(res_max_corr, 2)}')    return features_to_keep, features_to_dropdef multicollinearity_tables(X):    corr_mat = round(X.corr()*100)    corr_mat = corr_mat.where(np.triu(np.ones(corr_mat.shape)).astype(bool))    np.fill_diagonal(corr_mat.values, np.nan)        corr_df = corr_mat.stack().reset_index()    corr_df.columns = ['Feature_1', 'Feature_2', 'Corr']    corr_df = corr_df.sort_values('Corr', ascending=False)    return corr_mat, corr_df# Calculates the Kolmogorv-Smirnov test statistic for each feature subset by predictand up vs. down# Used to measure predictive power of feature as K-S test statistic is a valid metric (symmetric, etc.)def features_ks_up_vs_down(X, y):    invalid_predictands = y.unique()[~pd.Series(y.unique()).isin(['Up', 'Down'])]    if len(invalid_predictands) > 0:        raise ValueError('invalid preditands:', invalid_predictands)            up_index = X.index[y == 'Up']    down_index = X.index[y == 'Down']        if len(up_index) == 0:        raise ValueError('No data in up')    elif len(down_index) == 0:        raise ValueError('No data in down')            def ks_up_vs_down(x):        return kstest(x[up_index], x[down_index])    ks_stat_up_vs_down = X.apply(ks_up_vs_down)    return ks_stat_up_vs_downdef pca_plots(X, save_dir=None):    # PCA    pred_df = X.dropna().copy()    pred_df_scaled = StandardScaler().fit_transform(pred_df)    pca = PCA()    pc_pred_df = pca.fit_transform(pred_df_scaled)        if save_dir is not None and not os.path.exists(save_dir):        os.makedirs(save_dir)        # Biplot    def biplot(score,coeff,labels=None):        xs = score[:,0]        ys = score[:,1]        n = coeff.shape[0]        scalex = 1.0/(xs.max() - xs.min())        scaley = 1.0/(ys.max() - ys.min())        plt.scatter(xs * scalex,ys * scaley, c='lightskyblue')        texts = []        for i in range(n):            plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)            if labels is None:                texts.append(plt.text(coeff[i,0]* 1, coeff[i,1] * 1, "Var"+str(i+1), c='black', ha='center', va='center', size=15))            else:                texts.append(plt.text(coeff[i,0]* 1, coeff[i,1] * 1, labels[i], c='black', ha='center', va='center', size=15))        # plt.xlim(-1,1)        # plt.ylim(-1,1)        plt.xlabel("PC{}".format(1))        plt.ylabel("PC{}".format(2))        plt.grid()                adjust_text(texts, only_move={'points':'y', 'texts':'y'}, arrowprops=dict(arrowstyle="->", color='black', lw=0.5))    plt.figure(figsize=(15, 13))    biplot(pc_pred_df[:,0:2],np.transpose(pca.components_[0:2,:]), labels=pred_df.columns)    if save_dir is not None:        plt.savefig(save_dir + 'pca_biplot.png')        plt.close()        # Screeplot    PC_values = np.arange(min(10 + 1, len(pca.explained_variance_ratio_)))    plt.figure()    plt.plot(PC_values, pca.explained_variance_ratio_[:10], 'ro-', linewidth=2)    plt.title('Scree Plot')    plt.xlabel('Principal Component')    plt.ylabel('Proportion of Variance Explained')    if save_dir is not None:        plt.savefig(save_dir +'pca_screeplot.png')        plt.close()        # Cumulative Screeplot    PC_values = np.arange(10+1)    plt.figure()    plt.plot(PC_values, [0] + list(np.cumsum(pca.explained_variance_ratio_[:10])), 'ro-', linewidth=2)    plt.title('Cumulative Scree Plot')    plt.xlabel('Principal Component')    plt.ylabel('Cumulative Proportion of Variance Explained')    if save_dir is not None:        plt.savefig(save_dir + 'pca_cumulative_screeplot.png')        plt.close()'''Global Optimization Maximum Relevance Minimum Redundancyweighted_corr : if passed, performs optimization at that redundancy trade-off,otherwise maximizes the relevance-rendancy ratio'''def optimization_mrmr(X, y, is_continuous=True, weighted_corr=None):    F = pd.Series(f_regression(X, y)[0] if is_continuous else f_classif(X, y)[0], index = X.columns)    abs_corr_mat = np.abs(X.corr()).clip(1e-5)    n = len(X.columns)    weights = np.repeat(1/n, n)    weights_and_k = np.concatenate([weights, [1]])        if weighted_corr is not None:        def neg_relevance(weights):            return weights.T.dot(F)                bounds = ((0, 1.),) * n        cons = [{'type': 'eq', 'fun': lambda x: sum(x) - 1 }]        cons.append({'type':'eq', 'fun': lambda x: x.T.dot(abs_corr_mat).dot(x) - weighted_corr })        optimization = minimize(neg_relevance, weights, method='SLSQP', bounds = bounds,                                options = {'disp':False, 'ftol': 1e-15, 'maxiter': 1e5} , constraints=cons)        weights = optimization.x                if not optimization.success:            print(optimization)        weights = pd.Series(weights, index=X.columns).sort_values(ascending=False)    else:        def neg_FCQ_score(weights):            relevance = F.dot(weights)            redundancy = weights.dot(abs_corr_mat).dot(weights)            return -relevance / redundancy                def redundancy(weights_and_k):            return weights_and_k[:-1].T.dot(abs_corr_mat).dot(weights_and_k[:-1])*100                bounds = ((0, 1.),) * n + ((0, np.inf),)        cons = [{'type': 'eq', 'fun': lambda x: sum(x[:-1]) - x[-1] }]        cons.append({'type':'eq', 'fun': lambda x: F.dot(x[:-1]) - 1 })        optimization = minimize(redundancy, weights_and_k, method='SLSQP', bounds = bounds,                                options = {'disp':False, 'ftol': 1e-15, 'maxiter': 1e5} , constraints=cons)        weights = optimization.x[:-1] / optimization.x[-1]                if not optimization.success:            print(optimization)        weights = pd.Series(weights, index=X.columns).sort_values(ascending=False)    return weightsdef feature_selection_suite(assets_set, predictand_name, test_start_date, max_corr=0.8, is_continuous=True):    proc_data_dir = '../Portfolios/'+assets_set+'/Assets_Data/Processed_Data/'    processed_df = pd.read_csv(proc_data_dir + predictand_name.lower() + '_processed_assets_data_ts_features.csv', parse_dates=['Date'])    train_df = processed_df[processed_df['Date'] < test_start_date]    train_df = train_df.drop(columns=['Ticker', 'Date'])    X_train, y_train = train_df.drop(columns=[predictand_name]).copy(), train_df[predictand_name].copy()    feature_selection_dir = '../Portfolios/'+assets_set+'/Feature_Selection/'    if not os.path.exists(feature_selection_dir):        os.makedirs(feature_selection_dir)    corr_dir = feature_selection_dir + '/Corr/'    if not os.path.exists(corr_dir):        os.makedirs(corr_dir)        corr_mat, corr_df = multicollinearity_tables(X_train)    corr_mat.to_csv(corr_dir+'corr_mat.csv')    corr_df.to_csv(corr_dir+'pairwise_corr_sorted.csv')    multicorr_coefs_df = pd.DataFrame({'Multicorr_Coefs' : multicorr_coefs(X_train)})    multicorr_coefs_df.to_csv(corr_dir+'multicorr_coefs.csv')        features_to_keep, features_to_drop = select_non_collinear(X_train, max_corr=max_corr)    max_corr_features_df = pd.DataFrame({'Features_to_Keep' : ' '.join([feature_to_keep for feature_to_keep in features_to_keep]),                                          'Features_to_Drop' : ' '.join([feature_to_drop for feature_to_drop in features_to_drop])}, index=['Feature_Name']).T    max_corr_features_df.to_csv(feature_selection_dir + 'max_corr_features.csv')        features_to_keep, features_to_drop = select_non_multicollinear(X_train, max_corr=max_corr)    max_multicorr_features_df = pd.DataFrame({'Features_to_Keep' : ' '.join([feature_to_keep for feature_to_keep in features_to_keep]),                                              'Features_to_Drop' : ' '.join([feature_to_drop for feature_to_drop in features_to_drop])}, index=['Feature_Name']).T    max_multicorr_features_df.to_csv(feature_selection_dir + 'max_multicorr_features.csv')    X_train = X_train[features_to_keep]        sub_corr_mat, sub_corr_df = multicollinearity_tables(X_train)    sub_corr_mat.to_csv(corr_dir+'subset_corr_mat.csv')    sub_corr_df.to_csv(corr_dir+'sub_setpairwise_corr_sorted.csv')    sub_multicorr_coefs_df = pd.DataFrame({'Multicorr_Coefs' : multicorr_coefs(X_train)})    sub_multicorr_coefs_df.to_csv(corr_dir+'subset_multicorr_coefs.csv')        if not is_continuous:        ks_stat_up_vs_down = features_ks_up_vs_down(X_train, y_train)        ks_stat_up_vs_down_df = pd.DataFrame(ks_stat_up_vs_down, columns=['KS_Test_Stat'])        ks_stat_up_vs_down_df.to_csv(feature_selection_dir + 'features_ks_stat_up_vs_down.csv')        print('Saved KS test statistic up vs. down for each feature')            pca_plots(X_train, save_dir=feature_selection_dir+'PCA/')    print('Saved all PCA plots')        mrmr_weights = optimization_mrmr(X_train, y_train, is_continuous=is_continuous)    mrmr_weights_df = pd.DataFrame(mrmr_weights, columns=['MRMR_Weights'])    mrmr_weights_df.to_csv(feature_selection_dir + 'features_MRMR_weights.csv')        mrmr_weights_df = pd.read_csv(feature_selection_dir + 'features_MRMR_weights.csv', index_col=0)    mrmr_weights = mrmr_weights_df['MRMR_Weights']    fig = plt.figure(figsize = (10, 5))    plt.bar(mrmr_weights.index, mrmr_weights)    plt.xlabel('Feature')    plt.ylabel('MRMR Weight')    plt.title('Features MRMR Weights')    plt.savefig(feature_selection_dir + 'features_MRMR_weights.png')    plt.close()                